{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8020fd",
   "metadata": {},
   "source": [
    "# Лабораторна робота 4.1\n",
    "## Машинний переклад EN → UK на базі Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a668b",
   "metadata": {},
   "source": [
    "## 1) Імпорт та перевірка середовища"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5859fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/Desktop/KPI/DATA_ANALYS/lab4/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.9\n",
      "PyTorch: 2.9.1+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import platform\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59fa73",
   "metadata": {},
   "source": [
    "## 2) Завантаження локального `ukr.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd1145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /home/kali/Desktop/KPI/DATA_ANALYS/lab4\n",
      "Looking for: /home/kali/Desktop/KPI/DATA_ANALYS/lab4/ukr.txt\n",
      "Go.\tЙди.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #6584257 (deniko)\n",
      "Hi.\tВітаю!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #414700 (deniko)\n",
      "Hi.\tПривіт.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3841503 (rmdas)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_FILE = BASE_DIR / \"ukr.txt\"\n",
    "\n",
    "print(\"Working dir:\", BASE_DIR.resolve())\n",
    "print(\"Looking for:\", DATA_FILE.resolve())\n",
    "assert DATA_FILE.exists(), \"❌ ukr.txt не знайдено\"\n",
    "\n",
    "def load_pairs_txt(path: Path, max_lines: int | None = None):\n",
    "    pairs = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            en, uk = parts[0].strip(), parts[1].strip()\n",
    "            if en and uk:\n",
    "                pairs.append((en, uk))\n",
    "    return pairs\n",
    "\n",
    "with DATA_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76a490",
   "metadata": {},
   "source": [
    "## 3) Підготовка даних (швидкий режим)\n",
    "Зменшуй `MAX_SAMPLES`, якщо все ще довго. Для CPU зазвичай 5k–10k — ок.\n",
    "Також можна підняти `MAX_STEPS`/`MAX_SAMPLES`, якщо є GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697f1162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pairs: 160049\n",
      "Using pairs: 8000\n",
      "After filtering: 7999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>uk</th>\n",
       "      <th>en_len</th>\n",
       "      <th>uk_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stand still!</td>\n",
       "      <td>Стій спокійно!</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where's my coat?</td>\n",
       "      <td>Де моє пальто?</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not done.</td>\n",
       "      <td>Я не скінчив.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom insisted on going.</td>\n",
       "      <td>Том наполіг на тому, щоб піти.</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In accepting the money, he lost the respect of...</td>\n",
       "      <td>Взявши гроші, він втратив повагу людей.</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                                       Stand still!   \n",
       "1                                   Where's my coat?   \n",
       "2                                      I'm not done.   \n",
       "3                             Tom insisted on going.   \n",
       "4  In accepting the money, he lost the respect of...   \n",
       "\n",
       "                                        uk  en_len  uk_len  \n",
       "0                           Стій спокійно!       2       2  \n",
       "1                           Де моє пальто?       3       3  \n",
       "2                            Я не скінчив.       3       3  \n",
       "3           Том наполіг на тому, щоб піти.       4       6  \n",
       "4  Взявши гроші, він втратив повагу людей.      11       6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SAMPLES = 8000  \n",
    "MAX_STEPS   = 1200 \n",
    "\n",
    "pairs = load_pairs_txt(DATA_FILE)\n",
    "print(\"All pairs:\", len(pairs))\n",
    "\n",
    "random.shuffle(pairs)\n",
    "pairs = pairs[:MAX_SAMPLES]\n",
    "print(\"Using pairs:\", len(pairs))\n",
    "\n",
    "df = pd.DataFrame(pairs, columns=[\"en\", \"uk\"]).drop_duplicates()\n",
    "df[\"en_len\"] = df[\"en\"].str.split().str.len()\n",
    "df[\"uk_len\"] = df[\"uk\"].str.split().str.len()\n",
    "\n",
    "df = df[(df.en_len <= 35) & (df.uk_len <= 35)].reset_index(drop=True)\n",
    "print(\"After filtering:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b48cb9",
   "metadata": {},
   "source": [
    "## 4) Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a02c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7840 Validation: 159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'uk'],\n",
       "        num_rows: 7840\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'uk'],\n",
       "        num_rows: 159\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_size = 0.02\n",
    "n = len(df)\n",
    "n_val = max(1, int(n * test_size))\n",
    "\n",
    "val_df = df.sample(n=n_val, random_state=42)\n",
    "train_df = df.drop(val_df.index).reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Validation:\", len(val_df))\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[\"en\",\"uk\"]]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[\"en\",\"uk\"]]),\n",
    "})\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a7d6c",
   "metadata": {},
   "source": [
    "## 5) Модель та токенізація (коротші послідовності = швидше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6148c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/Desktop/KPI/DATA_ANALYS/lab4/venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map:   0%|                                                                                   | 0/7840 [00:00<?, ? examples/s]/home/kali/Desktop/KPI/DATA_ANALYS/lab4/venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 7840/7840 [00:00<00:00, 11648.55 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 159/159 [00:00<00:00, 7803.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7840\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 159\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-uk\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_source_len = 64\n",
    "max_target_len = 64\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(batch[\"en\"], max_length=max_source_len, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"uk\"], max_length=max_target_len, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = ds.map(preprocess, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e16465",
   "metadata": {},
   "source": [
    "## 6) Тренування (без eval/save кожні N кроків)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaefe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239463/2452178967.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "/home/kali/Desktop/KPI/DATA_ANALYS/lab4/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 15:01, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.566940336227417, metrics={'train_runtime': 902.5551, 'train_samples_per_second': 10.636, 'train_steps_per_second': 1.33, 'total_flos': 27753226960896.0, 'train_loss': 0.566940336227417, 'epoch': 1.2244897959183674})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mt_en_uk_fast20\",\n",
    "\n",
    "    max_steps=MAX_STEPS,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    predict_with_generate=False,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8 if torch.cuda.is_available() else 4,\n",
    "    gradient_accumulation_steps=2, \n",
    "    logging_steps=50,\n",
    "    fp16=use_fp16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd70727",
   "metadata": {},
   "source": [
    "## 7) Швидка оцінка BLEU на піднаборі (наприклад 200 прикладів)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e617704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 56.27799372538043,\n",
       " 'counts': [836, 540, 366, 246],\n",
       " 'totals': [1054, 895, 736, 577],\n",
       " 'precisions': [79.3168880455408,\n",
       "  60.33519553072626,\n",
       "  49.72826086956522,\n",
       "  42.63431542461005],\n",
       " 'bp': 0.9971577470606117,\n",
       " 'sys_len': 1054,\n",
       " 'ref_len': 1057}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def fast_bleu_eval(n_samples=200, num_beams=4, max_new_tokens=64):\n",
    "    val_raw = ds[\"validation\"]\n",
    "    n = min(n_samples, len(val_raw))\n",
    "    idx = list(range(len(val_raw)))\n",
    "    random.shuffle(idx)\n",
    "    idx = idx[:n]\n",
    "\n",
    "    sources = [val_raw[i][\"en\"] for i in idx]\n",
    "    refs = [[val_raw[i][\"uk\"]] for i in idx]\n",
    "\n",
    "    model.eval()\n",
    "    batch_size = 16 if torch.cuda.is_available() else 8\n",
    "    preds = []\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = sources[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_source_len).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, num_beams=num_beams, max_new_tokens=max_new_tokens)\n",
    "        preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "\n",
    "    preds = [p.strip() for p in preds]\n",
    "    refs = [[r[0].strip()] for r in refs]\n",
    "\n",
    "    res = bleu.compute(predictions=preds, references=refs)\n",
    "    return res\n",
    "\n",
    "res = fast_bleu_eval(n_samples=200)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea1fb6",
   "metadata": {},
   "source": [
    "## 8) Приклади перекладу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7129b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: I love Ukraine and I want to learn the language.\n",
      "UK: Я люблю Україну і хочу вивчити мову.\n",
      "------------------------------------------------------------\n",
      "EN: Cybersecurity is important for modern organizations.\n",
      "UK: Кібербезпеки важливі для сучасних організацій.\n",
      "------------------------------------------------------------\n",
      "EN: Where is the nearest train station?\n",
      "UK: Де найближча залізнична станція?\n",
      "------------------------------------------------------------\n",
      "EN: Please open the window.\n",
      "UK: Будь ласка, відчиніть вікно.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def translate(sentences, num_beams=4, max_new_tokens=80):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_source_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, num_beams=num_beams, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "test_sents = [\n",
    "    \"I love Ukraine and I want to learn the language.\",\n",
    "    \"Cybersecurity is important for modern organizations.\",\n",
    "    \"Where is the nearest train station?\",\n",
    "    \"Please open the window.\",\n",
    "]\n",
    "\n",
    "for s, t in zip(test_sents, translate(test_sents)):\n",
    "    print(\"EN:\", s)\n",
    "    print(\"UK:\", t)\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30af64a",
   "metadata": {},
   "source": [
    "## 9) (Опціонально) Збереження моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8f6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/Desktop/KPI/DATA_ANALYS/lab4/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61586]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: /home/kali/Desktop/KPI/DATA_ANALYS/lab4/mt_en_uk_fast20_saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SAVE_DIR = Path(\"mt_en_uk_fast20_saved\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(SAVE_DIR))\n",
    "tokenizer.save_pretrained(str(SAVE_DIR))\n",
    "\n",
    "print(\"✅ Saved to:\", SAVE_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5aa434",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "- Виконано машинний переклад EN→UK на основі трансформера.\n",
    "- Застосовано fine-tuning у «швидкому» режимі (~20 хв на CPU).\n",
    "- BLEU пораховано на піднаборі validation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab4 venv",
   "language": "python",
   "name": "lab4_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
